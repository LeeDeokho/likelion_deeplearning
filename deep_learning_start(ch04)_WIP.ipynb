{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 시작하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chap 4. 신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망으로 어떻게 데이터를 학습시킬까를 공부해 볼 수 있는 차례가 왔습니다. 신경망에서 학습이란 cost 함수를 줄여주는 weight값을 찾아가는 과정입니다.\n",
    "\n",
    "여기서 cost 함수 개념이 도입됩니다. 어떻게 weight값들을 줄어 나갈 수 있을까요~?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 데이터에서 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망을 representation learning이라고도 부른다. 이유는 데이터를 보고 학습할 수 있기 때문.\n",
    "- 신경망이 데이터를 보고 weight값을 조정해서 cost 함수를 줄여나간다. \n",
    "\n",
    "- 신경망이 깊어질수록 weight값들을 증가한다. 당연히 뉴런과 뉴런을 이어주는 시냅스가 증가하기 때문! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 데이터 주도학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기계학습은 데이터에서 패턴을 찾고 데이터로 이야기 하는 것\n",
    "- 신경망과 딥러닝은 기존 머신러닝에서 사용하던 방법보다 사람의 개입을 더욱 배제하길 원하는(?) 특징을 가지고 있다. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (내의견) 기존 머신러닝은 feature engineer이 정말 중요한 요소였다. 데이터 전처리 작업이라고 부르는데 이 작업이 머신러닝 작업에서 70~80%를 차지할 정도로 비중이 큰 작업이었다. 실제 알고리즘을 돌려보는 작업은 전체작업의 20%정도밖에 안될 정도로 tidy하고 clean한 데이터를 기계학습 알고리즘에 넣어주는 것이 중요했다. 물론 이 방법이 아직도 유효하고 머신러닝에 정말 중요한 이슈이다. 하지만 딥러닝은 end-to-end learning이라고 부른다. 데이터를 넣어주고 feature를 추출해주는 작업까지 해주는 것이다. 하지만 딥러닝에 넣어주는 데이터도 tidy하고 clean할 수록 성능이 높아지는건 예외가 아닐 것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5를 인식하는 알고리즘을 예제로 들겠다. \n",
    "- 사람이 보기에는 5라는 숫자를 인식하는데 어려움이 없지만 기계는 어떤 알고리즘으로 5를 인식시키게 할지 방법이 필요하다.\n",
    "- 한가지 방법으로 이미지에서 feature를 추출한다.\n",
    "- 이미지의 특징은 보통 벡터로 표현한다.\n",
    "- 영상처리 분야에서 feature engineering기법으로 SIFT, SURF, HOG등의 방법이 쓰인다.\n",
    "- 이러한 기법으로 feature를 추출하고 SVM, KNN등으로 학습시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지를 벡터로 변환하는 작업은 역시 '사람'이 하는 작업이다.\n",
    "- 좋은 feature를 추출하지 못하면 좋은 성능을 내지 못한다.\n",
    "- 딥러닝 이전까지 기계학습은 이러한 feature 추출이 주된 일이었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 훈련 데이터와 시험 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기계학습에서 데이터를 training set과 test set으로 나누어 학습과 테스트를 진행한다\n",
    "- training set을 이용해서 최적의 파라미터를 찾는다.\n",
    "- test set을 이용해 trained 된 모델을 테스트한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋을 나누는 이유는?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 범용적으로 사용할 수 있는 모델을 만들기 위해서!\n",
    "- genaralization 시키기 위해 test set을 분리하는 것이다.\n",
    "- 범용적이라는 말은 아직 마주치치 못한 데이터에 대해서도 모델의 성능을 발휘하기 위해서이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 오버피팅이란?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training set에만 지나치게 최적화된 상태를 overfitting이라 한다.\n",
    "- 오버피팅은 기계학습의 매우매우 중요한 과제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적의 파라미터 값을 찾는 지표로 손실함수(cost function)을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 평균제곱오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 많이 쓰이는 손실함수로 mse(평균제곱오차) 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 소프트맥스 함수의 출력 => 확률로 해석할 수 있으므로 0일 확률은 0.1, 1일 확률은 0.05 ...이다.\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "# 숫자 0부터 9까지 정답 레이블이다. 여기서 숫자 2가 정답이다.\n",
    "# 한 원소만 1로 하고 그 외 값은 1로 하는 것을 one-hot encoding이라 한다.\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평균제곱오차 함수 구현\n",
    "각 원소의 출력값과 정답 레이블의 차를 제곱한 후, 그 총합을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 출력(추정 값)이 '2'인 경우에 가장 높다. 즉, 출력과 정답이 같을 때 에러가 적다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59750000000000003"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 정답이 2일때\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 정답은 똑같이 '2'지만 신경망의 출력은 7이므로 에러가 높다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59750000000000003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답이 7일 확률이 높다고 추정했을 경우\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 교차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            E = -sigma(t_k * log(y_k))\n",
    "\n",
    "- t_k는 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0이다.(원-핫 인코딩)\n",
    "- 정답일 떄만 출력(추정 값)의 자연로그를 계산하는 식!\n",
    "- ex) 정답이 '2'일때, 출력(추정 값)이 0.6 일떄, 교차 엔트로피 오차는 -log(0.6) = 0.51\n",
    "- ex) 정답이 '2'일때, 출력(추정 값)이 0.1 일때, 교차 엔트로피 오차는 -log(0.1) = 2.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결론, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 교차 엔트로피 함수 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- y와 t는 넘파이 배열\n",
    "- np.log를 계산할 때 아주 작은 값인 delta를 더함 ==> np.log()에 0을 입력하면 마이너스 무한대를 뜻하는 -inf가 되서 그걸 방지하기 위해서!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entory_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정답이 '2' 일때, 출력이 0.6인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51082545709933802"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "\n",
    "cross_entory_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정답이 '2' 일때, 출력이 0.1인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025840929945458"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entory_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기계학습은 training set을 통해 학습을 한다. \n",
    "- training set에 대한 손실함수 값을 구하고, 그 값을 줄여주는 파라미터를 찾아낸다.\n",
    "- 그러므로, 모든 training set을 대상으로 손실 함수 값을 구해야한다.\n",
    "- 만약, 데이터가 100개일 때, 100개의 손실 함수 값들의 합을 지표로 삼는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training set 모두에 대한 손실 함수의 값을 구해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                 E = -1/N * sigma_n(sigma_k(t_nk * log(y_nk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터의 총 합은 N\n",
    "- t_nk == n번째 데이터의 k차원 째의 값\n",
    "- 데이터 하나의 손실함수를 N개의 데이터로 확장 한것.\n",
    "- N으로 나누어 '평균 손실 함수'를 구한다.\n",
    "- 평균 손실 함수를 쓰면 데이터의 갯수와 상관없이 언제든 통일된 지표를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런데, 데이터가 엄청엄청 많아지면 다 계산해야 하나??\n",
    "- 현실적이지 않다. 많은 데이터 중에 일부를 샘플링해서 전체의 '근사치'로 사용한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   #### 이를 미니배치(mini_batch)라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "sys.path.insert(0, './deep-learning-from-scratch/')\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training set은 60,000개, 784차원(28 X 28)이다.\n",
    "- 이 중, 무작위로 10개만 추출하려면 어떻게 하면 될까?\n",
    "- np.random.choice()로 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "print(train_size)\n",
    "print(x_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 (배치용) 교차 엔트로피 오차 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 데이터의 교차 엔트로피 함수를 구해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- y는 신경망의 출력, t는 정답 레이블\n",
    "- 데이터가 하나이면, np.reshape로 데이터의 형상을 바꾸고 이미지 1장당 교차 엔트로피 오차를 계산\n",
    "- 배치 사이즈 만큼 나눠줘서 평균 교차 엔트로피를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y) / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정답 레이블이 원-핫 인코딩이 아닌 '2'나 '7'등의 숫자 레이블로 주어졌을 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 코드의 핵심은 t가 0 인 원소는 교차 엔트로피 에러도 0이므로 그 계산은 무시해도 된다.\n",
    "- 그래서, 위 코드와 다른 점은 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "return -np.sum(t * np.log(y) / batch_size)\n",
    "\n",
    "return -np.sum(np.log(y[np.arange(batch_size), t]) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.2.5 왜 손실함수를 설정하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망 학습에서 최적의 매개변수를 탐색할때 손실함수의 값을 가능한 작게하는 파라미터 값을 찾는다.\n",
    "- 이때, 파라미터늬 미분(기울기)를 계산하고,그 미분값을 단서로 파라미터의 값을 갱신한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 신경망의 어느 한 가중치 파라미터에 주목한다고 하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치 파라미터의 손실함수의 미분이란 '가중치 매개변수의 값을 아주 조금 변화 시켰을때, 손실함수가 어떻게 변하냐' 이다\n",
    "- 만약,  이 미분값이 음수면, 가중치 파라미터를 양의 방향으로 변화시켜 손실 함수를 줄일 수 있다.\n",
    "- 반대로, 이 미분값이 양수면, 가중치 파라미터를 음의 방향으로 변화시켜 손실 함수를 줄일 수 있다.\n",
    "- 하지만, 미분 값이 0이면 가중치 파라미터를 어느쪽으로 움직여도 손실 함수의 값을 달라지지 않는다. 이 지점을 찾는것이 관건!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 활성화 함수로 계단함수를 사용하면 같은 이유로 학습이 잘 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파라미터의 작은 변화가 주는 파장을 계단함수가 죽이기 때문에 손실함수에 아무 변화가 일어나지 않는다.\n",
    "- 하지만, 시그모이드 함수는 어느 지점에서든 기울기가 0이 아니기 때문에 이러한 문제가 일어나지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 수치미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-52ce1744e0cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numeric_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 계산한 미분 값이 x에 대한 f(x)의 변화량이다.\n",
    "- 수치적 접근 vs 해석적 접근 차이를 알면된다. 여기서 구하는 것은 수치 미분이고 우리가 미적분 시간에 한건 해석적 미분이다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 편미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0 ** 2.0 + x1*x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 복잡해보이지만, 동장방식은 변수가 하나일때 수치미분과 거의 같다.\n",
    "- function_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 앞에서 구한 function_2의 그라디언트를 구한다.\n",
    "\n",
    "- (6.000000, 7.9999999)로 값이 나오지만 넘파이는 보기 쉽게 소수점을 생략하고 출력한다.\n",
    "- 세 점의 그라디언트를 각각 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3.0 , 4.0)점의 그라디언트를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  8.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (0, 2) 점의 그라디언트를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  4.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3, 0) 점의 그라디언트를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기는 각 지점에서 낮아지는 방향을 가리킨다. 즉, 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 줄이는 방향이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/gradient.jpeg\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.4.1 경사법(경사 하강법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 계속 반복되는 말이지만, 신경망의 목표는 최적의 파라미터(가중치와 편향)을 찾아 학습하는 것이다.\n",
    "- 여기서 최적이란, 손실 함수가 최솟값이 될떄의 파라미터 값이다.\n",
    "- 그러나, 파라미터 공간이 너무 광대해서 어디가 최솟값이 되는지 찾기가 쉽지않다....\n",
    "- 이런 상황에서, 기울기를 잘 이용해서 손실함수의 최솟값을 찾으려는 것이 경사법이다..!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의할 점, 기울기가 가리키는 곳이 정말 함수의 최솟값이 존재하는지, 그 쪽이 정말 나아가야할 방향인지 보장할 수 없다.\n",
    "#### 실제로 복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없는 경우가 대부분이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수가 local minimum이라고 global minimum이 될 수 있는지 장담 할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법(수식)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 수식은 갱신하는 양을 나타낸다.\n",
    "- 수학용어 에타는 learning rate(학습률)이라고 부른다.\n",
    "- 아래 식은 1회에 해당하는 갱신이다. 이를 반복한다.\n",
    "- 변수의 값을 갱신하는 단계를 여러번 반복하며, 서서히 함수의 값을 줄인다.\n",
    "- learning rate는 0.01 이나 0.001등 미리 특정값(하이퍼파라미터)으로 정해둬야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/gd.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
